{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154dd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "class AvitoScraper:\n",
    "    BASE_URL = \"https://www.avito.ma\"\n",
    "    HEADERS = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, output_file=\"avito_cars.xlsx\"):\n",
    "        self.output_file = output_file\n",
    "        \n",
    "    def _get_page_url(self, page_number):\n",
    "        return f\"{self.BASE_URL}/fr/maroc/voiture?o={page_number}\" if page_number > 1 else f\"{self.BASE_URL}/fr/maroc/voiture\"\n",
    "    \n",
    "    def _extract_car_data(self, car):\n",
    "        params = {}\n",
    "        for param in car.get('params', {}).get('secondary', []):\n",
    "            params[param.get('key')] = param.get('value', 'null')\n",
    "        \n",
    "        location = car.get(\"location\", \"null\")\n",
    "        \n",
    "        return {\n",
    "            \"id\": car.get(\"id\", \"null\"),\n",
    "            \"list_id\": car.get(\"listId\", \"null\"),\n",
    "            \"title\": car.get(\"subject\", \"null\"),\n",
    "            \"description\": car.get(\"description\", \"null\"),\n",
    "            \"ad_type\": car.get(\"adType\", {}).get(\"label\", \"null\"),\n",
    "            \"category\": car.get(\"category\", {}).get(\"formatted\", \"null\"),\n",
    "            \"price\": car.get(\"price\", {}).get(\"value\", \"null\"),\n",
    "            \"currency\": car.get(\"price\", {}).get(\"currency\", \"null\"),\n",
    "            \"location\": location,\n",
    "            \"city\": location.split(\",\")[0].strip() if location != \"null\" else \"null\",\n",
    "            \"date_posted\": car.get(\"date\", \"null\"),\n",
    "            \"seller_name\": car.get(\"seller\", {}).get(\"name\", \"null\"),\n",
    "            \"seller_type\": car.get(\"seller\", {}).get(\"type\", \"null\"),\n",
    "            \"seller_verified\": car.get(\"seller\", {}).get(\"isVerifiedSeller\", False),\n",
    "            \"brand\": params.get(\"brand\", \"null\"),\n",
    "            \"model\": params.get(\"model\", \"null\"),\n",
    "            \"year\": params.get(\"regdate\", \"null\"),\n",
    "            \"mileage\": params.get(\"mileage\", \"null\"),\n",
    "            \"fuel_type\": params.get(\"fuel\", \"null\"),\n",
    "        }\n",
    "    \n",
    "    def _process_page(self, page_number):\n",
    "        url = self._get_page_url(page_number)\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.HEADERS)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            script = soup.find('script', id='__NEXT_DATA__')\n",
    "            \n",
    "            if not script:\n",
    "                print(f\"No script tag found on page {page_number}\")\n",
    "                return []\n",
    "                \n",
    "            data = json.loads(script.string)\n",
    "            cars = data[\"props\"][\"pageProps\"][\"initialReduxState\"][\"ad√π*\"][\"search\"][\"ads\"]\n",
    "            return [self._extract_car_data(car) for car in cars]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {page_number}: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape(self, start_page=1, end_page=1):\n",
    "        all_cars = []\n",
    "        \n",
    "        for page in range(start_page, end_page + 1):\n",
    "            page_cars = self._process_page(page)\n",
    "            all_cars.extend(page_cars)\n",
    "            print(f\"Page {page} done: {len(page_cars)} cars found\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "        self._save_to_excel(all_cars)\n",
    "        return all_cars\n",
    "    \n",
    "    def _save_to_excel(self, new_data):\n",
    "        if os.path.exists(self.output_file):\n",
    "            existing_data = pd.read_excel(self.output_file)\n",
    "            all_data = pd.concat([existing_data, pd.DataFrame(new_data)], ignore_index=True)\n",
    "        else:\n",
    "            all_data = pd.DataFrame(new_data)\n",
    "        \n",
    "        all_data.to_excel(self.output_file, index=False)\n",
    "        print(f\"Saved {len(all_data)} cars to Excel (added {len(new_data)} new records)!\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "scraper = AvitoScraper()\n",
    "cars_data = scraper.scrape(start_page=1, end_page=200)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
