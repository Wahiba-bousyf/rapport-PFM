{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9e7f24-92fd-40e7-bb2b-a23e6323e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import os\n",
    "from openpyxl.utils.exceptions import IllegalCharacterError\n",
    "from zipfile import BadZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190afcd-b403-4ec9-9bfc-761f0894bd10",
   "metadata": {},
   "source": [
    "# Scraping cars data from Avito.ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae138c4-b5eb-419b-9388-892e896ca606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 501 done: 35 cars found\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_cars\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Scrape data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m cars_data = \u001b[43mscrape_cars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_page\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m501\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_page\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m700\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Save to JSON first\u001b[39;00m\n\u001b[32m     85\u001b[39m json_file = \u001b[33m\"\u001b[39m\u001b[33mavito_voitures_2024.json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mscrape_cars\u001b[39m\u001b[34m(start_page, end_page)\u001b[39m\n\u001b[32m     71\u001b[39m         all_cars.append(car_info)\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m done: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(cars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cars found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError on page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://www.avito.ma\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "def clean_string(value):\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    if isinstance(value, (int, float)):\n",
    "        return value\n",
    "    \n",
    "    value = str(value)\n",
    "    # Remove null bytes and control characters\n",
    "    value = ''.join(char for char in value if 31 < ord(char) or char in '\\t\\n\\r')\n",
    "    # Replace other problematic characters with space\n",
    "    value = value.replace('\\x00', ' ').replace('\\r', ' ').replace('\\n', ' ')\n",
    "    # Remove any remaining control characters\n",
    "    value = ''.join(char for char in value if char.isprintable() or char in '\\t\\n\\r')\n",
    "    return value.strip()\n",
    "\n",
    "def scrape_cars(start_page=1, end_page=1):\n",
    "    all_cars = []\n",
    "    \n",
    "    for page in range(start_page, end_page + 1):\n",
    "        url = f\"{BASE_URL}/fr/maroc/voiture?o={page}\" if page > 1 else f\"{BASE_URL}/fr/maroc/voiture\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            script = soup.find('script', id='__NEXT_DATA__')\n",
    "            \n",
    "            if not script:\n",
    "                print(f\"No script tag found on page {page}\")\n",
    "                continue\n",
    "                \n",
    "            data = json.loads(script.string)\n",
    "            cars = data[\"props\"][\"pageProps\"][\"initialReduxState\"][\"ad\"][\"search\"][\"ads\"]\n",
    "            \n",
    "            for car in cars:\n",
    "                params = {}\n",
    "                for param in car.get('params', {}).get('secondary', []):\n",
    "                    params[param.get('key')] = param.get('value', 'null')\n",
    "                \n",
    "                car_info = {\n",
    "                    \"id\": clean_string(car.get(\"id\", \"null\")),\n",
    "                    \"list_id\": clean_string(car.get(\"listId\", \"null\")),\n",
    "                    \"title\": clean_string(car.get(\"subject\", \"null\")),\n",
    "                    \"description\": clean_string(car.get(\"description\", \"null\")),\n",
    "                    \"ad_type\": clean_string(car.get(\"adType\", {}).get(\"label\", \"null\")),\n",
    "                    \"category\": clean_string(car.get(\"category\", {}).get(\"formatted\", \"null\")),\n",
    "                    \"price\": clean_string(car.get(\"price\", {}).get(\"value\", \"null\")),\n",
    "                    \"currency\": clean_string(car.get(\"price\", {}).get(\"currency\", \"null\")),\n",
    "                    \"location\": clean_string(car.get(\"location\", \"null\")),\n",
    "                    \"city\": clean_string(car.get(\"location\", \"null\").split(\",\")[0].strip() if car.get(\"location\") else \"null\"),\n",
    "                    \"date_posted\": clean_string(car.get(\"date\", \"null\")),\n",
    "                    \"seller_name\": clean_string(car.get(\"seller\", {}).get(\"name\", \"null\")),\n",
    "                    \"seller_type\": clean_string(car.get(\"seller\", {}).get(\"type\", \"null\")),\n",
    "                    \"seller_verified\": car.get(\"seller\", {}).get(\"isVerifiedSeller\", False),\n",
    "                    \"brand\": clean_string(params.get(\"brand\", \"null\")),\n",
    "                    \"model\": clean_string(params.get(\"model\", \"null\")),\n",
    "                    \"year\": clean_string(params.get(\"regdate\", \"null\")),\n",
    "                    \"mileage\": clean_string(params.get(\"mileage\", \"null\")),\n",
    "                    \"fuel_type\": clean_string(params.get(\"fuel\", \"null\")),\n",
    "                    \"transmission\": clean_string(params.get(\"transmission\", \"null\")),\n",
    "                    \"url\": clean_string(urljoin(BASE_URL, car.get(\"href\", \"\"))) if car.get(\"href\") else \"null\",\n",
    "                    \"is_premium\": car.get(\"isPremium\", False),\n",
    "                    \"is_urgent\": car.get(\"isUrgent\", False),\n",
    "                }\n",
    "                \n",
    "                all_cars.append(car_info)\n",
    "            \n",
    "            print(f\"Page {page} done: {len(cars)} cars found\")\n",
    "            time.sleep(2)  \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {page}: {e}\")\n",
    "    \n",
    "    return all_cars\n",
    "\n",
    "# Scrape data\n",
    "cars_data = scrape_cars(start_page=501, end_page=700)\n",
    "\n",
    "# Save to JSON first\n",
    "json_file = \"avito_voitures_2024.json\"\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(cars_data, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Data saved to {json_file}\")\n",
    "\n",
    "# Load from JSON and clean\n",
    "df_from_json = pd.read_json(json_file)\n",
    "\n",
    "# Apply cleaning to all string columns\n",
    "for col in df_from_json.select_dtypes(include=['object']).columns:\n",
    "    df_from_json[col] = df_from_json[col].apply(clean_string)\n",
    "\n",
    "output_file = \"avito_cars.xlsx\"\n",
    "\n",
    "try:\n",
    "    # Try to save to Excel first\n",
    "    df_from_json.to_excel(output_file, index=False, engine='openpyxl')\n",
    "    print(f\"Successfully saved {len(df_from_json)} cars to Excel!\")\n",
    "    \n",
    "except IllegalCharacterError:\n",
    "    print(\"Encountered illegal characters - trying alternative cleaning method\")\n",
    "    # More aggressive cleaning for Excel\n",
    "    for col in df_from_json.select_dtypes(include=['object']).columns:\n",
    "        df_from_json[col] = df_from_json[col].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "    \n",
    "    try:\n",
    "        df_from_json.to_excel(output_file, index=False, engine='openpyxl')\n",
    "        print(f\"Successfully saved {len(df_from_json)} cars to Excel after aggressive cleaning!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save Excel file after cleaning: {e}\")\n",
    "        csv_file = \"avito_cars_failed.csv\"\n",
    "        df_from_json.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Saved data to {csv_file} instead\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save Excel file: {e}\")\n",
    "    csv_file = \"avito_cars_failed.csv\"\n",
    "    df_from_json.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved data to {csv_file} instead\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
